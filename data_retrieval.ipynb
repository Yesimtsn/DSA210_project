{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Video-IDs of my Watch History into an Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 32525 video IDs.\n"
     ]
    }
   ],
   "source": [
    "with open('watch-history.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "watch_hist_ids = []  #List to store video IDs and watched times\n",
    "watched_time_map = {}  #Dictionary to map video ID to its watched time\n",
    "\n",
    "for item in data:\n",
    "    if 'titleUrl' in item:\n",
    "        url = item['titleUrl']\n",
    "        \n",
    "        if 'v=' in url:\n",
    "            watch_hist_id = url.split('v=')[1].split('&')[0]  #Extract video ID\n",
    "            watched_time = item.get('time')  #Extract watched timestamp\n",
    "            \n",
    "            if watch_hist_id and watched_time:\n",
    "                watch_hist_ids.append(watch_hist_id)  #Store video ID\n",
    "                watched_time_map[watch_hist_id] = watched_time  #Map video ID to watched time\n",
    "\n",
    "print(f\"Extracted {len(watch_hist_ids)} video IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Meta-Data of the Videos Watched:\n",
    "*Category ID*, \n",
    "*Watch Date*, \n",
    "*Duration Time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the YouTube API client\n",
    "API_KEY = 'AIzaSyA7MfRXQKG53Zn9JAnwSE-yC7KL044j4us'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "def parse_duration(duration_str):\n",
    "    pattern = r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\"\n",
    "    match = re.match(pattern, duration_str)\n",
    "    \n",
    "    hours = int(match.group(1) or 0)\n",
    "    minutes = int(match.group(2) or 0)\n",
    "    seconds = int(match.group(3) or 0)\n",
    "    \n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "def fetch_video_details(ids,map):\n",
    "    all_hist_data = []  # List to store metadata\n",
    "    batch_size = 50  # Maximum batch size allowed by API\n",
    "    total_batches = (len(ids) + batch_size - 1) // batch_size  # Total number of batches\n",
    "\n",
    "    for i in range(0, len(ids), batch_size):\n",
    "        batch = ids[i:i + batch_size]\n",
    "        try:\n",
    "            # Fetch details for the current batch\n",
    "            request = youtube.videos().list(part='snippet,contentDetails', id=','.join(batch))\n",
    "            response = request.execute()\n",
    "\n",
    "            # Extract relevant metadata\n",
    "            for item in response['items']:\n",
    "                video_id = item['id']\n",
    "                duration =parse_duration(item['contentDetails']['duration'])\n",
    "                video_data = {\n",
    "                    'categoryId': item['snippet']['categoryId'],\n",
    "                    'watched_at': map.get(video_id, 'Unknown'),  # Add watched time from map\n",
    "                    'durationv':duration\n",
    "                }\n",
    "                all_hist_data.append(video_data)\n",
    "\n",
    "            print(f\"Batch {i // batch_size + 1}/{total_batches} fetched successfully.\")\n",
    "            time.sleep(1)  # Pause to avoid hitting rate limits\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch {i // batch_size + 1}: {e}\")\n",
    "            time.sleep(5)  # Pause before retrying\n",
    "\n",
    "    return all_hist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting the Fetched Data into a JSON File for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_metadata = fetch_video_details(watch_hist_ids,watched_time_map)\n",
    "with open('video_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(video_metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Fetched data for {len(video_metadata)} videos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating JSON Files for Each Year and Putting Related Data into the JSON Files for Better Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "with open(\"video_metadata.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Parse data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'watched_at' to datetime\n",
    "df['watched_at'] = df['watched_at'].apply(parse_date)\n",
    "df['watched_at'] = pd.to_datetime(df['watched_at'], errors='coerce')\n",
    "\n",
    "# Extract year and month for grouping\n",
    "df['year'] = df['watched_at'].dt.year\n",
    "df['month'] = df['watched_at'].dt.month\n",
    "df['count'] = 1\n",
    "\n",
    "# Group by year, month, and category to calculate total duration\n",
    "grouped = (\n",
    "    df.groupby(['year', 'month', 'categoryId'])\n",
    "    .agg(\n",
    "        total_duration=('durationv', 'sum'),  # Sum of video durations\n",
    "        category_count=('count', 'sum')  # Count of videos per category\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Convert total duration to minutes\n",
    "grouped['total_duration'] = grouped['total_duration'] / 60.0\n",
    "\n",
    "# Prepare the yearly data\n",
    "for year in df['year'].dropna().unique():\n",
    "    yearly_data = []\n",
    "    for month in range(1, 13):  # Iterate over all months\n",
    "        month_data = grouped[(grouped['year'] == year) & (grouped['month'] == month)]\n",
    "        if not month_data.empty:\n",
    "            # Create the dictionaries for category counts and category durations\n",
    "            categories_count = month_data[['categoryId', 'category_count']].set_index('categoryId').to_dict()['category_count']\n",
    "            categories_duration = month_data[['categoryId', 'total_duration']].set_index('categoryId').to_dict()['total_duration']\n",
    "            \n",
    "            # Prepare the month summary\n",
    "            month_summary = {\n",
    "                \"month\": month,\n",
    "                \"total_duration_minutes\": float(month_data['total_duration'].sum()),  # Total duration in minutes\n",
    "                \"categories_count\": {str(k): int(v) for k, v in categories_count.items()},  # Category counts\n",
    "                \"categories_duration_minutes\": {str(k): float(v) for k, v in categories_duration.items()}  # Duration per category in minutes\n",
    "            }\n",
    "            yearly_data.append(month_summary)\n",
    "    \n",
    "    # Write to a yearly JSON file\n",
    "    with open(f\"video_metadata{int(year)}.json\", \"w\") as year_file:\n",
    "        json.dump(yearly_data, year_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
